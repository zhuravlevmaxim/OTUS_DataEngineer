{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StringType}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current spark version is 2.4.4\n"
     ]
    }
   ],
   "source": [
    "println(s\"Current spark version is ${spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputStreamPath = /home/jovyan/data/events-stream\n",
       "modelPath = /home/jovyan/models/spark-ml-model\n",
       "dataSchema = StructType(StructField(tweet,StringType,true))\n",
       "inputDF = [tweet: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tweet: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputStreamPath = \"/home/jovyan/data/events-stream\"\n",
    "val modelPath = \"/home/jovyan/models/spark-ml-model\"\n",
    "\n",
    "val dataSchema = new StructType()\n",
    "    .add(\"tweet\", StringType)\n",
    "\n",
    "val inputDF = spark\n",
    "    .readStream\n",
    "    .schema(dataSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .json(inputStreamPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataSchema = StructType(StructField(target,IntegerType,true), StructField(id,LongType,true), StructField(raw_timestamp,StringType,true), StructField(query_status,StringType,true), StructField(author,StringType,true), StructField(tweet,StringType,true))\n",
       "dataPath = /home/jovyan/data/training.1600000.processed.noemoticon.csv\n",
       "raw_sentiment = [label: int, tweet: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, tweet: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, IntegerType, LongType, StringType}\n",
    "\n",
    "val dataSchema = new StructType()\n",
    "    .add(\"target\", IntegerType)\n",
    "    .add(\"id\", LongType)\n",
    "    .add(\"raw_timestamp\", StringType)\n",
    "    .add(\"query_status\", StringType)\n",
    "    .add(\"author\", StringType)\n",
    "    .add(\"tweet\", StringType)\n",
    "\n",
    "val dataPath= \"/home/jovyan/data/training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "val raw_sentiment = spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\",false)\n",
    "    .schema(dataSchema)\n",
    "    .load(dataPath)\n",
    "    .selectExpr(\"(case when target=4 then 1 else 0 end) as label\",\"tweet\").randomSplit(Array(0.1, 0.9))(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer = tok_4b3b043a4377\n",
       "hashingTF = hashingTF_67123d0f9190\n",
       "labelIndexer = strIdx_850e3c870c65\n",
       "featureIndexer = vecIdx_bedcdf745067\n",
       "trainingData = [label: int, tweet: string]\n",
       "testData = [label: int, tweet: ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[label: int, tweet: ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer, IndexToString, StringIndexer, VectorIndexer}\n",
    "\n",
    "val tokenizer = new Tokenizer()\n",
    "    .setInputCol(\"tweet\")\n",
    "    .setOutputCol(\"words\")\n",
    "val hashingTF = new HashingTF()\n",
    "    .setNumFeatures(1000)\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"tweet\")\n",
    "  .setOutputCol(\"indexedLabel\")\n",
    "val featureIndexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexedFeatures\")\n",
    "  .setMaxCategories(10)\n",
    "val Array(trainingData, testData) = raw_sentiment.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"indexedLabel\")\n",
    "  .setFeaturesCol(\"indexedFeatures\")\n",
    "  .setNumTrees(10)\n",
    "\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "//   .setLabels(labelIndexer.labels)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, hashingTF, labelIndexer, featureIndexer, rf, labelConverter\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in thread \"Executor task launch worker for task 128\" java.lang.SecurityException: Not allowed to invoke System.exit!\n",
      "\tat org.apache.toree.security.KernelSecurityManager.checkExit(KernelSecurityManager.scala:147)\n",
      "\tat java.lang.Runtime.halt(Runtime.java:273)\n",
      "\tat org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:32)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:613)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 128, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n",
       "  at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)\n",
       "  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)\n",
       "  at org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:142)\n",
       "  at org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:120)\n",
       "  at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
       "  at scala.util.Try$.apply(Try.scala:192)\n",
       "  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
       "  at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:120)\n",
       "  at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
       "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
       "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
       "  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\n",
       "  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\n",
       "  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\n",
       "  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n",
       "  ... 44 elided"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write.overwrite().save(\"/home/jovyan/models/spark-ml-my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sameModel = pipeline_fc19f65b89cc\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_fc19f65b89cc"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sameModel = PipelineModel.load(\"/home/jovyan/models/spark-ml-my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionsDF = [tweet: string, words: array<string> ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[tweet: string, words: array<string> ... 4 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsDF = sameModel.transform(inputDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getProbability = UserDefinedFunction(<function1>,DoubleType,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,DoubleType,Some(List(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val getProbability = udf((prediction: org.apache.spark.ml.linalg.Vector) => prediction(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6f7e466f"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         negative|   10|\n",
      "|         positive|    5|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         undefine|    4|\n",
      "|         negative|    9|\n",
      "|         positive|    3|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         undefine|    5|\n",
      "|         negative|    7|\n",
      "|         positive|    4|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         negative|    8|\n",
      "|         positive|    8|\n",
      "|         undefine|    4|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         negative|    5|\n",
      "|         positive|    8|\n",
      "|         undefine|    2|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         negative|    5|\n",
      "|         positive|    5|\n",
      "|         undefine|    2|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         negative|    6|\n",
      "|         positive|    6|\n",
      "|         undefine|    3|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         positive|    5|\n",
      "|         negative|    5|\n",
      "|         undefine|    1|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         positive|    5|\n",
      "|         negative|    5|\n",
      "|         undefine|    1|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         positive|   10|\n",
      "|         negative|    6|\n",
      "|         undefine|    1|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         positive|    5|\n",
      "|         negative|    5|\n",
      "|         undefine|    2|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         undefine|    4|\n",
      "|         positive|   14|\n",
      "|         negative|    9|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         undefine|    1|\n",
      "|         positive|   10|\n",
      "|         negative|    3|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         undefine|    2|\n",
      "|         positive|    3|\n",
      "|         negative|    5|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         positive|   14|\n",
      "|         negative|    5|\n",
      "+-----------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|clean_probability|count|\n",
      "+-----------------+-----+\n",
      "|         negative|   11|\n",
      "|         undefine|    1|\n",
      "|         positive|   12|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>\n",
    "    batchDF.select(getProbability($\"probability\").alias(\"clean_probability\"))\n",
    "    .selectExpr(\"\"\"\n",
    "    CASE\n",
    "        WHEN clean_probability > 0.55 THEN 'positive'\n",
    "        WHEN clean_probability < 0.45 THEN 'negative'\n",
    "        ELSE 'undefine'\n",
    "    END AS clean_probability\n",
    "    \"\"\")\n",
    "    .withColumn(\"time_stamp\", lit(current_timestamp()))\n",
    "    .withWatermark(\"time_stamp\", \"20 seconds\")\n",
    "    .groupBy(window($\"time_stamp\", \"10 seconds\").alias(\"time_stamp\"),\n",
    "            $\"clean_probability\")\n",
    "    .count()\n",
    "    .select($\"clean_probability\", $\"count\")\n",
    "    .show()\n",
    "}.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
